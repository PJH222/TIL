{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f5c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e8c330",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=country-student-count, master=local) created by __init__ at /tmp/ipykernel_4785/1077266209.py:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry-student-count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# SparkContext의 변수명은 웬만하면 sc로 만드는 것을 권장\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m sc\n",
      "File \u001b[0;32m~/miniconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/context.py:350\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    347\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;241m%\u001b[39m (currentAppName, currentMaster,\n\u001b[1;32m    355\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction, callsite\u001b[38;5;241m.\u001b[39mfile, callsite\u001b[38;5;241m.\u001b[39mlinenum))\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=country-student-count, master=local) created by __init__ at /tmp/ipykernel_4785/1077266209.py:7 "
     ]
    }
   ],
   "source": [
    "# SparkConf : 스파크 실행 환경 설정 클래스\n",
    "# SparkContext : Driver program 실행 환경 구성을 위한 클래스\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"country-student-count\")\n",
    "\n",
    "# SparkContext의 변수명은 웬만하면 sc로 만드는 것을 권장\n",
    "sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724c197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"/home/ubuntu/working/spark/data/xAPI-Edu-Data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc05a9",
   "metadata": {},
   "source": [
    "# 2. 데이터 로딩\n",
    "- 로컬이든, HDFS에서 데이터를 불러오든 RDD로 데이터를 추상화 시킨다.\n",
    "- 추상화를 \"대충 말하기~\" 라고 대충 이해하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "461fcf10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:////home/ubuntu/working/spark/data/xAPI-Edu-Data.csv MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(f\"file:///{filepath}\")\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882488c6",
   "metadata": {},
   "source": [
    "# 3. 데이터 확인\n",
    "- Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "436ced3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gender,NationalITy,PlaceofBirth,StageID,GradeID,SectionID,Topic,Semester,Relation,raisedhands,VisITedResources,AnnouncementsView,Discussion,ParentAnsweringSurvey,ParentschoolSatisfaction,StudentAbsenceDays,Class'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = lines.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a410d929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 헤더를 제외한 모든 데이터 불러오기\n",
    "datas = lines.filter(lambda row : row != header)\n",
    "datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96cb3685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[12] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = datas.map(lambda row : row.split(\",\")[2])\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b82c5836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KuwaIT', 'KuwaIT', 'KuwaIT']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries.collect()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "305cf939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'KuwaIT': 142,\n",
       "             'lebanon': 11,\n",
       "             'Egypt': 3,\n",
       "             'SaudiArabia': 8,\n",
       "             'USA': 16,\n",
       "             'Jordan': 15,\n",
       "             'venzuela': 1,\n",
       "             'Iran': 4,\n",
       "             'Tunis': 8,\n",
       "             'Morocco': 4,\n",
       "             'Syria': 1})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cb4a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "# 일 다끝나면 꼭 종료 시켜주기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98382f96",
   "metadata": {},
   "source": [
    "### 오늘 공부해야 할 내용 \n",
    "1. Spark가 왜 In Memory 연산을 하는가?\n",
    "2. RDD의 개요가 무엇인가\n",
    " - Lazy Evaluation의 의미\n",
    " - Resilient 의 의미\n",
    " - DAG, Transfomations, Actions의 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4bc415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
