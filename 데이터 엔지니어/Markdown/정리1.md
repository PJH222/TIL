[노션 링크](https://radial-fighter-931.notion.site/Hadoop-994ea138f23c46e0bebe00d79981e37c)

```XML
<configuration>
        <property>
                <name>dfs.replication</name>
                <value>1</value>
        </property>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/home/ubuntu/hadoop-3.2.4/dfs/name</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/home/ubuntu/hadoop-3.2.4/dfs/data</value>
        </property>
        <property>
                <name>dfs.namenode.http-address</name>
                <value>ec2-18-177-112-132.ap-northeast-1.compute.amazonaws.com:9870</value>
        </property>
</configuration>
```

##### yarn-site.xml

```xml
vi $HADOOP_HOME/etc/hadoop/yarn-site.xml
```

```xml
<configuration>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
        <property>
                <name>yarn.nodemanager.env-whitelist</name>
                <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>
        </property>
        <property>
                <name>yarn.resourcemanager.webapp.address</name>
                <value>ec2-18-177-112-132.ap-northeast-1.compute.amazonaws.com:8088</value>
        </property>
        <property>
                <name>yarn.nodemanager.webapp.address</name>
                <value>ec2-18-177-112-132.ap-northeast-1.compute.amazonaws.com:8042</value>
        </property>
<!-- Site specific YARN configuration properties -->

</configuration>
```


```xml
vi $HADOOP_HOME/etc/hadoop/mapred-site.xml
```

##### 이 부분은 수정할 내용 없음
혹시나 잘 안된다면, 
파일탐색기를 통해
/home/ubuntu/hadoop-3.2.4/etc/hadoop
들어가서 파일하나하나 찾아서 바꿔주면 된다.

1. http://ec2-18-177-112-132.ap-northeast-1.compute.amazonaws.com:9870/dfshealth.html#tab-datanode

2. http://ec2-18-177-112-132.ap-northeast-1.compute.amazonaws.com:8088/cluster

3. http://ec2-18-177-112-132.ap-northeast-1.compute.amazonaws.com:19888/jobhistory


`hdfs dfs -mkdir /user/ubuntu/test`
-- 안되면 

"`hdfs dfs -mkdir -p /user/ubuntu/test`"로 입력할 것

`mysql -umulti -p1111 < employees.sql`

괄호 안에 있는 것은 생략해서 작성하면 된다.
```
$SQOOP_HOME/bin/sqoop import \
--connect 'jdbc:mysql://ec2-18-177-112-132.ap-northeast-1.compute.amazonaws.com(host 주소)/employees(DB 이름)?useUnicode=true&serverTimezone=Asia/Seoul' \
--username multi \
--password 1111 \
--query 'SELECT e.emp_no, e.birth_date, e.first_name, e.last_name, e.gender, e.hire_date, d.dept_no FROM employees e, dept_emp d WHERE (e.emp_no = d.emp_no) AND $CONDITIONS' \
--target-dir /user/ubuntu/sqoop/employees \
--split-by e.emp_no
```

- 수정 완료
```
$SQOOP_HOME/bin/sqoop import \
--connect 'jdbc:mysql://ec2-18-177-112-132.ap-northeast-1.compute.amazonaws.com/employees?useUnicode=true&serverTimezone=Asia/Seoul' \
--username multi \
--password 1111 \
--query 'SELECT e.emp_no, e.birth_date, e.first_name, e.last_name, e.gender, e.hire_date, d.dept_no FROM employees e, dept_emp d WHERE (e.emp_no = d.emp_no) AND $CONDITIONS' \
--target-dir /user/ubuntu/sqoop/employees \
--split-by e.emp_no
```

- 파일 내용 출력하기
`hdfs dfs -cat /user/ubuntu/sqo
op/employees`
cat: `/user/ubuntu/sqoo

`hdfs dfs -ls /user/ubuntu/sqoop/employees`
얘네가 무엇을 의미하는가?

- mysql 로그인
`mysql -umulti -p1111`

- 아까와는 달리 이번에는 export임, 하둡 -> DB로
```
$SQOOP_HOME/bin/sqoop export --connect 'jdbc:mysql://ec2-18-177-112-132.ap-northeast-1.compute.amazonaws.com/employees?useUnicode=true&serverTimezone=Asia/Seoul' \
--username multi \
--password 1111 \
--table export_employees \
--export-dir /user/ubuntu/sqoop/employees \
--input-fields-terminated-by ',' --input-lines-terminated-by '\n' \
--columns emp_no,birth_date,first_name,last_name,gender,hire_date,dept_no
```

- path/to 변경
```xml
export FLUME_HOME=/path/to/flume-1.9.0
export PATH=$PATH:$FLUME_HOME/bin
```

```html
export FLUME_HOME=/path/to/flume-1.9.0
export PATH=$PATH:$FLUME_HOME/bin
```

- socket
vim sample-agent-socket.conf

```xml
$FLUME_HOME/bin/flume-ng agent \
-n socket_sample_agent \
-c $FLUME_HOME/conf \
-f $FLUME_HOME/conf/sample-agent-socket.conf \
-Dflume.root.logger=INFO,console
```

- `-n` : 에이전트의 이름
- `-c` : 설정 파일(`conf` 파일)들이 들어 있는 디렉토리 지정
- `-f` : 설정 파일
- `-Dflume.root.logger` : 플룸 로깅 어플리케이션의 설정

```
(base) ubuntu@ip-172-31-2-100:~$ cd working/flume/batch-data-samples/
(base) ubuntu@ip-172-31-2-100:~/working/flume/batch-data-samples$ echo "hello Flume!!" >> log1.txt
echo "hello Flumecd working/flume/batch-data-samples/" >> log1.txt
(base) ubuntu@ip-172-31-2-100:~/working/flume/batch-data-samples$ 
```
'>> log1.txt' 하면
아래 처럼 반응함

```
2024-03-22 14:15:47,837 (pool-3-thread-1) [INFO - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.rollCurrentFile(ReliableSpoolingFileEventReader.java:497)] Preparing to move file /home/ubuntu/working/flume/batch-data-samples/log1.txt to /home/ubuntu/working/flume/batch-data-samples/log1.txt.COMPLETED
2024-03-22 14:15:49,326 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 68 65 6C 6C 6F 20 46 6C 75 6D 65 63 64 20 77 6F hello Flumecd wo }
2024-03-22 14:17:01,385 (pool-3-thread-1) [INFO - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.readEvents(ReliableSpoolingFileEventReader.java:384)] Last read took us just up to a file boundary. Rolling to the next file, if there is one.
2024-03-22 14:17:01,386 (pool-3-thread-1) [INFO - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.rollCurrentFile(ReliableSpoolingFileEventReader.java:497)] Preparing to move file /home/ubuntu/working/flume/batch-data-samples/log2.txt to /home/ubuntu/working/flume/batch-data-samples/log2.txt.COMPLETED
2024-03-22 14:17:01,387 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 61 73 64 73 64 73 61 73 64         
```

- hdfs 연결시 주의할 점
자동적으로 Flume이라는 유저가 HDFS로 데이터를 보낸다~~ 가 된다.
- 이후에, Flume 유저에게 권한을 부여한다는 의미

`hdfs dfs -mkdir /user/flume`
`hdfs dfs -chown flume /user/flume`
`- chown` 특정 권한(소유권) 부여

(base) ubuntu@ip-172-31-2-100:~/working/flume/batch-data-samples$ echo "hihihihihihi" >> hdfs_log1.log



### 스터디 할 내용
1. SQOOP 설치 및 사용
  - 기존 교재에 있는 쿼리 말고 새로운 쿼리 만들어서 사용해 보기
  - 새로운 쿼리를 사용 했을 때 잘 안되면 말씀 주세요
  - import 위주로 실습 할 것! ( rdbms -> hdfs )

2. FLUME을 이용한 로그 처리 실습 다시 해보기
  - 디렉토리 이름 변경 및 설정 파일 변경 등 실습 해보기

3. HDFS 구조와 이론 공부하기
  - CLI 명령어 다시 사용해보기